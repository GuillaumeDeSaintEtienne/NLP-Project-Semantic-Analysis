{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "16a50a2c",
   "metadata": {},
   "source": [
    "\n",
    "# Job Recommendation System ‚Äî Global Notebook\n",
    "\n",
    "```mermaid\n",
    "gantt\n",
    "    dateFormat  YYYY-MM-DD\n",
    "    title       Job Recommendation Project ‚Äî 2-week Plan\n",
    "    excludes    weekends\n",
    "\n",
    "    section Week 1 - Setup & Data\n",
    "    Planning & Kickoff          :    planning, 2025-10-01, 2d\n",
    "    Data gathering & cleaning   :active,  data,    2025-10-03, 3d\n",
    "    Quick EDA and schema review :         eda,     2025-10-06, 1d\n",
    "\n",
    "    section Week 1-2 - Core NLP\n",
    "    Text preprocessing & mapping:         preprocess, 2025-10-07, 2d\n",
    "    Model experiments (MiniLM / mpnet):crit, model, 2025-10-07, 4d\n",
    "    Embedding pipeline & scoring :         pipeline, 2025-10-09, 2d\n",
    "\n",
    "    section Week 2 - Frontend & Integration\n",
    "    Streamlit frontend dev      :         frontend, 2025-10-10, 2d\n",
    "    Integration (NLP ‚Üî Frontend) :         integrate, 2025-10-10, 2d\n",
    "    Testing & bugfixing         :         test,      2025-10-11, 1d\n",
    "\n",
    "    section Finalization\n",
    "    Documentation & notebook    :milestone, docs, 2025-10-12, 1d\n",
    "    Final review & presentation :         final,    2025-10-12, 1d\n",
    "```\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a676b998",
   "metadata": {},
   "source": [
    "\n",
    "## Imports and Model Initialization\n",
    "\n",
    "These imports include all necessary libraries for data manipulation, text preprocessing, and semantic similarity computation using **Sentence-BERT (SBERT)**.  \n",
    "NLTK is used for text tokenization and stopword removal, while Plotly is used for visualization.\n",
    "\n",
    "### Model Choices :\n",
    "During experimentation, we tested two Sentence-BERT (SBERT) models:\n",
    "\n",
    "- `all-MiniLM-L6-v2` ‚Äì a lightweight, multilingual-friendly model known for its speed and small memory footprint.\n",
    "\n",
    "- `all-mpnet-base-v2` ‚Äì a larger and more powerful English-only model with higher embedding quality and semantic accuracy.\n",
    "\n",
    "**We finally chose `all-mpnet-base-v2` because it provides better semantic similarity performance, especially for nuanced English text.\n",
    "Although it is heavier and not multilingual, its higher accuracy and contextual understanding make it more suitable for precise job‚Äìprofile matching, where the quality of embeddings has a strong impact on the ranking results.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "676f4055",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import re\n",
    "import plotly.graph_objects as go\n",
    "import nltk\n",
    "from pathlib import Path\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import math\n",
    "\n",
    "# Download required NLTK resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Model initialization\n",
    "print(\"Loading SBERT model...\")\n",
    "MODEL_ID = 'all-mpnet-base-v2'\n",
    "print(\"Model loaded.\")\n",
    "\n",
    "data_path = Path.cwd() / \"data\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb61d474",
   "metadata": {},
   "source": [
    "\n",
    "## Function: `graph_result(jobScores)`\n",
    "\n",
    "**Purpose:**  \n",
    "Creates a **polar radar chart** using Plotly to visualize how well a user's profile matches different job roles.\n",
    "\n",
    "**Input:**  \n",
    "- `jobScores`: list of tuples `(job_title, score, top_skills)` representing the job name, similarity score, and top matching skills.\n",
    "\n",
    "**Output:**  \n",
    "- Returns a Plotly `Figure` object ready for visualization.\n",
    "\n",
    "**Role in Project:**  \n",
    "Used at the end of the NLP pipeline to graphically represent how strongly the user matches each potential job.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "738cdaf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def graph_result(jobScores):\n",
    "    fig = go.Figure()\n",
    "\n",
    "    bestJobsTitle = [job for job, score, topSkills in jobScores]\n",
    "    bestJobsScoresValues = [round(score * 100, 2) for job, score, topSkills in jobScores]\n",
    "\n",
    "    maxScore = max(bestJobsScoresValues)\n",
    "    minScore = min(bestJobsScoresValues)\n",
    "\n",
    "    fig = go.Figure(data=go.Scatterpolar(\n",
    "        r=bestJobsScoresValues,\n",
    "        theta=bestJobsTitle,\n",
    "        fill='toself',\n",
    "        name='Profile Match'\n",
    "    ))\n",
    "    fig.update_layout(\n",
    "        polar=dict(radialaxis=dict(visible=True, range=[minScore - 4, maxScore + 1])),\n",
    "        title=\"Overall Job Profile Match (Weighted)\"\n",
    "    )\n",
    "    \n",
    "    return fig\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b50b3a12",
   "metadata": {},
   "source": [
    "\n",
    "## Function: `loadData()`\n",
    "\n",
    "**Purpose:**  \n",
    "Loads the competency and job datasets from the `data/` folder.\n",
    "\n",
    "**Input:**  \n",
    "- None (uses relative paths).\n",
    "\n",
    "**Output:**  \n",
    "- `df_competencies`: DataFrame containing all competencies.\n",
    "- `df_jobs`: DataFrame containing job descriptions and required competencies.\n",
    "\n",
    "**Role in Project:**  \n",
    "Provides the base data for the recommendation system. Each job is associated with several competencies.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05bbf1b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def loadData():\n",
    "    try:\n",
    "        df_competencies = pd.read_csv(data_path / r\"competencies.csv\", sep=\",\")\n",
    "        df_jobs = pd.read_csv(data_path / r\"jobs.csv\", sep=\",\")\n",
    "        df_jobs[\"RequiredCompetencies\"] = df_jobs[\"RequiredCompetencies\"].apply(lambda x: x.split(\";\"))\n",
    "    except FileNotFoundError:\n",
    "        print(\"Error: Make sure 'competencies.csv' and 'jobs.csv' are in a 'data' folder at the project root.\")\n",
    "        df_competencies = pd.DataFrame()\n",
    "        df_jobs = pd.DataFrame()\n",
    "    return df_competencies, df_jobs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf79c0c2",
   "metadata": {},
   "source": [
    "\n",
    "## Function: `transformInDf(...)`\n",
    "\n",
    "**Purpose:**  \n",
    "Transforms user input (skills, experience, interests) into a pandas DataFrame suitable for NLP processing.\n",
    "\n",
    "**Input:**  \n",
    "- User profile data (skills levels, tools, experience, etc.).\n",
    "\n",
    "**Output:**  \n",
    "- A single-row DataFrame representing the user's answers.\n",
    "\n",
    "**Role in Project:**  \n",
    "Acts as the bridge between the Streamlit interface and the NLP engine, converting form input into structured data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffae70dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def transformInDf(level_python, level_ai, level_visu, level_sql, level_token_embedding,\n",
    "                  tools, languages, frameworks, data_types, preferred_domains,\n",
    "                  experience_text, challenges, learning_goals):\n",
    "    data = {\n",
    "        \"level_python\": level_python,\n",
    "        \"level_ai\": level_ai,\n",
    "        \"level_visu\": level_visu,\n",
    "        \"level_sql\": level_sql,\n",
    "        \"level_token_embedding\": level_token_embedding,\n",
    "        \"tools\": tools,\n",
    "        \"languages\": languages,\n",
    "        \"frameworks\": frameworks,\n",
    "        \"data_types\": data_types,\n",
    "        \"preferred_domains\": preferred_domains,\n",
    "        \"experience_text\": experience_text,\n",
    "        \"challenges\": challenges,\n",
    "        \"learning_goals\": learning_goals\n",
    "    }\n",
    "    df = pd.DataFrame([data])\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c4ee8f3",
   "metadata": {},
   "source": [
    "\n",
    "## Function: `clean_text(text)`\n",
    "\n",
    "**Purpose:**  \n",
    "Cleans and normalizes text for embedding.\n",
    "\n",
    "**Input:**  \n",
    "- `text`: Raw user input text.\n",
    "\n",
    "**Output:**  \n",
    "- Cleaned and tokenized string with stopwords removed.\n",
    "\n",
    "**Role in Project:**  \n",
    "Ensures consistent input for SBERT embedding and similarity calculation.\n",
    "\n",
    "**Technical Choice:**<br>\n",
    "We chose not to use lemmatization to avoid losing contextual meaning.<br>\n",
    "Since Sentence-BERT captures the semantic representation of full sentences, modifying words to their base forms could slightly change the intended meaning.\n",
    "Keeping the original text ensures the embeddings reflect the user's phrasing and context more accurately.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03b28257",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"[^a-zA-Z0-9\\s]\", \" \", text)\n",
    "    # We uses NLTK's word_tokenize and stopwords for better tokenization and stopword removal\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [t for t in tokens if t not in stopwords.words('english') and len(t) > 2]\n",
    "    return \" \".join(tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "368a3e50",
   "metadata": {},
   "source": [
    "\n",
    "## Function: `preprocessing(df)`\n",
    "\n",
    "**Purpose:**  \n",
    "Maps numeric skill levels to descriptive phrases and cleans all text columns.\n",
    "\n",
    "**Input:**  \n",
    "- `df`: DataFrame containing the user profile data.\n",
    "\n",
    "**Output:**  \n",
    "- Preprocessed DataFrame ready for sentence embedding.\n",
    "\n",
    "**Role in Project:**  \n",
    "Transforms structured numeric and text inputs into natural language sentences for semantic embedding.\n",
    "\n",
    "**Explanation:**\n",
    "Transform numerical skill levels into descriptive sentences and clean all text fields.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfa701aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def preprocessing(df):\n",
    "    mappingLevel = {\n",
    "        1: \"Beginner\",\n",
    "        2: \"Novice\",\n",
    "        3: \"Intermediate\",\n",
    "        4: \"Advanced\",\n",
    "        5: \"Expert\"\n",
    "    }\n",
    "    mappingCompetence = {\n",
    "        \"level_python\": \"python\",\n",
    "        \"level_ai\": \"Artificial Intelligence\",\n",
    "        \"level_visu\": \"Visualization\",\n",
    "        \"level_sql\": \"SQL\",\n",
    "        \"level_token_embedding\": \"Tokenization and embeddings\"\n",
    "    }\n",
    "    mappingExperience = {\n",
    "        \"experience_text\": \"My experience includes:  \",\n",
    "        \"tools\": \"I have used these tools and software:  \",\n",
    "        \"languages\": \"I know the following programming languages: \",\n",
    "        \"frameworks\": \"I am familiar with these frameworks and libraries: \",\n",
    "        \"data_types\": \"I have worked with these types of data: \",\n",
    "        \"preferred_domains\": \"I am interested in these domains: \"\n",
    "    }\n",
    "\n",
    "    for col in df.columns:\n",
    "        if pd.api.types.is_integer_dtype(df[col]):\n",
    "            df[col] = df[col].apply(lambda numLevel: f\"I am a {mappingLevel.get(numLevel)} in {mappingCompetence.get(col)}\")\n",
    "        elif pd.api.types.is_string_dtype(df[col]):\n",
    "            if col in mappingExperience.keys():\n",
    "                df[col] = df[col].apply(lambda x: f\"{mappingExperience[col]} {x}\" if pd.notnull(x) and x != \"\" else \"\")\n",
    "            df[col] = df[col].fillna(\"\").apply(clean_text)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72bce7de",
   "metadata": {},
   "source": [
    "\n",
    "## Function: `nlp(...)`\n",
    "\n",
    "**Purpose:**  \n",
    "The **core NLP engine** ‚Äî combines embeddings, computes similarity, applies weighting, and generates job recommendations.\n",
    "\n",
    "**Input:**  \n",
    "- All user answers (skill levels, text fields, etc.).\n",
    "\n",
    "**Output:**  \n",
    "- A dictionary containing:\n",
    "  - `\"fig\"` ‚Üí Polar chart (Plotly figure).\n",
    "  - `\"top_jobs\"` ‚Üí List of top 3 matching jobs.\n",
    "  - `\"block_scores\"` ‚Üí Dictionary of competency block coverage.\n",
    "\n",
    "**Role in Project:**  \n",
    "Executes the entire recommendation pipeline:\n",
    "1. Converts user input to natural text.  \n",
    "2. Embeds user data and competencies using SBERT.  \n",
    "3. Computes similarity scores and weights them by job relevance (IDF).  \n",
    "4. Returns final ranked job recommendations and visualizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4be206b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nlp(level_python, level_ai, level_visu, level_sql, level_token_embedding,\n",
    "        tools, languages, frameworks, data_types, preferred_domains,\n",
    "        experience_text, challenges, learning_goals):\n",
    "\n",
    "    # --- STEP 1: Create a dataframe from user input ---\n",
    "    # Convert all input parameters (skills, tools, experience, etc.) into a single-row DataFrame.\n",
    "    df_question = transformInDf(level_python, level_ai, level_visu, level_sql, level_token_embedding,\n",
    "                                tools, languages, frameworks, data_types, preferred_domains,\n",
    "                                experience_text, challenges, learning_goals)\n",
    "    \n",
    "    # --- STEP 2: Preprocess the text data ---\n",
    "    # Transform numerical skill levels into descriptive sentences and clean all text fields.\n",
    "    df_question = preprocessing(df_question)\n",
    "\n",
    "    # --- STEP 3: Load competencies and job datasets ---\n",
    "    df_competencies, df_jobs = loadData()\n",
    "\n",
    "    # --- STEP 4: Compute IDF scores for competencies ---\n",
    "    # IDF (Inverse Document Frequency) gives more weight to rare or specialized competencies.\n",
    "    total_jobs = len(df_jobs)\n",
    "\n",
    "    # Count how often each competency appears across all jobs.\n",
    "    competency_counts = df_jobs.explode('RequiredCompetencies')['RequiredCompetencies'].value_counts().to_dict()\n",
    "\n",
    "    # Define a helper function to compute the IDF for each competency.\n",
    "    def calculate_idf(competency_id):\n",
    "        count = competency_counts.get(competency_id, 0)\n",
    "        return math.log(total_jobs / (count + 1))  # add 1 to avoid division by zero\n",
    "\n",
    "    # Apply the IDF function to all competencies.\n",
    "    df_competencies['idf_score'] = df_competencies['CompetencyID'].apply(calculate_idf)\n",
    "\n",
    "    # --- STEP 5: Load the Sentence-BERT model ---\n",
    "    # This model transforms text into numerical embeddings (semantic vectors).\n",
    "    model = SentenceTransformer(MODEL_ID)\n",
    "\n",
    "    # --- STEP 6: Encode the user profile and competency text ---\n",
    "    # Convert the user‚Äôs text profile into a vector representation.\n",
    "    listQuestion = df_question.iloc[0]\n",
    "    userEmbedding = model.encode(listQuestion, convert_to_tensor=True)\n",
    "\n",
    "    # Encode all competencies and competency block names (skill categories).\n",
    "    compEmbeddings = model.encode(df_competencies[\"Competency\"].tolist(), convert_to_tensor=True)\n",
    "    blockEmbeddings = model.encode(df_competencies[\"BlockName\"].unique().tolist(), convert_to_tensor=True)\n",
    "\n",
    "    # --- STEP 7: Compute cosine similarity between user and competencies ---\n",
    "    # The cosine similarity measures how close the user's profile is to each competency. (-1 to 1)\n",
    "    compCosineMatrix = util.cos_sim(userEmbedding, compEmbeddings).cpu().numpy()\n",
    "    compCosineScores = compCosineMatrix.max(axis=0)  # take the highest similarity per competency\n",
    "    df_competencies[\"similarity\"] = compCosineScores\n",
    "\n",
    "    # --- STEP 8: Compute similarity at the block (category) level ---\n",
    "    blockCosineMatrix = util.cos_sim(userEmbedding, blockEmbeddings).cpu().numpy()\n",
    "    blockCosineScores = blockCosineMatrix.max(axis=0)\n",
    "\n",
    "    # --- STEP 9: Compute a weighted score per competency ---\n",
    "    # Combines the competency similarity and block-level similarity for better weighting.\n",
    "    df_competencies[\"weightedScore\"] = df_competencies.apply(\n",
    "        lambda row: row[\"similarity\"] * (1 + 0.1 * blockCosineScores[row['BlockID'] - 1]),\n",
    "        axis=1\n",
    "    )\n",
    "\n",
    "    # --- STEP 10: Aggregate scores by block for visualization ---\n",
    "    scoresByBlock = df_competencies.groupby(\"BlockName\")[\"weightedScore\"].mean().to_dict()\n",
    "\n",
    "    # --- STEP 11: Compute job recommendation scores ---\n",
    "    jobScores = []\n",
    "    for _, job in df_jobs.iterrows():\n",
    "        # Filter competencies required for this job.\n",
    "        jobComps = df_competencies[df_competencies[\"CompetencyID\"].isin(job[\"RequiredCompetencies\"])]\n",
    "\n",
    "        if not jobComps.empty:\n",
    "            # Weighted average of similarity scores, adjusted by IDF.\n",
    "            weighted_job_score = (jobComps['weightedScore'] * jobComps['idf_score']).sum()\n",
    "            sum_of_idf = jobComps['idf_score'].sum()\n",
    "            jobScore = weighted_job_score / sum_of_idf if sum_of_idf > 0 else 0\n",
    "\n",
    "            # Identify top 3 matching competencies for this job.\n",
    "            topCompScores = jobComps.sort_values(by='weightedScore', ascending=False).head(3)\n",
    "        else:\n",
    "            # If the job has no competencies, assign a score of 0.\n",
    "            jobScore = 0\n",
    "            topCompScores = pd.DataFrame(columns=[\"Competency\"])\n",
    "\n",
    "        # Save the job title, score, and best-matching skills.\n",
    "        jobScores.append((job[\"JobTitle\"], jobScore, topCompScores[\"Competency\"].tolist()))\n",
    "\n",
    "    # Sort jobs by descending similarity score.\n",
    "    jobScores.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # --- STEP 12: Select top 3 recommended jobs ---\n",
    "    top3Jobs = []\n",
    "    for job, score, topSkills in jobScores[:3]:\n",
    "        top3Jobs.append({\n",
    "            \"title\": job,\n",
    "            \"score\": round(score * 100, 2),  # convert to percentage\n",
    "            \"matching_skills\": topSkills\n",
    "        })\n",
    "\n",
    "    # --- STEP 13: Generate visualization ---\n",
    "    # Create a radar/polar chart showing the user's match across all jobs.\n",
    "    fig = graph_result(jobScores)\n",
    "\n",
    "    # --- STEP 14: Return all computed results ---\n",
    "    return {\n",
    "        \"fig\": fig,                  # Polar chart visualization\n",
    "        \"top_jobs\": top3Jobs,        # Top 3 recommended jobs\n",
    "        \"block_scores\": scoresByBlock  # Average score per competency block\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fdbf749",
   "metadata": {},
   "source": [
    "\n",
    "# Streamlit Frontend (`front.py`)\n",
    "\n",
    "Below is the full Streamlit code that defines the web interface for user input and visualization of job recommendations.  \n",
    "It collects user information, sends it to the `nlp()` function, and displays results (top 3 jobs, polar chart, and competency bars).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3d6e3f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import streamlit as st\n",
    "import plotly.graph_objects as go\n",
    "import pandas as pd\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Get the absolute path of the current file's directory (interface/)\n",
    "current_dir = os.path.dirname(os.path.abspath(__file__))\n",
    "# Get the path of the parent directory (the project root)\n",
    "project_root = os.path.dirname(current_dir)\n",
    "# Add the project root to the system's path\n",
    "sys.path.append(project_root)\n",
    "\n",
    "# Now, Python can find the NLP module which is in a sibling directory\n",
    "from NLP import main\n",
    "\n",
    "def start():\n",
    "    st.set_page_config(page_title=\"Job Finder\", page_icon=\"üíº\", layout=\"wide\")\n",
    "    st.title(\"Find Your Ideal Job üîé\")\n",
    "    st.write(\"Answer a few questions and find out the best job opportunities based on your profile.\")\n",
    "\n",
    "    with st.form(\"job_form\"):\n",
    "        st.header(\"üë§ Your Profile\")\n",
    "\n",
    "        # Using columns for a cleaner layout\n",
    "        col1, col2 = st.columns(2)\n",
    "        with col1:\n",
    "            st.subheader(\"üìä Rate Your Skills (1-Beginner, 5-Expert)\")\n",
    "            level_python = st.slider(\"How much do you love python ?\", 1, 5, 3)\n",
    "            level_ai = st.slider(\"Do you like working with AI ?\", 1, 5, 2)\n",
    "            level_visu = st.slider(\"Can you make art with data ?\", 1, 5, 1)\n",
    "            level_sql = st.slider(\"How confident are you concerning your knoledge in SQL ?\", 1, 5, 2)\n",
    "            level_token_embedding = st.slider(\"How familiar are you with tokkenization and embeddings ?\", 1, 5, 1)\n",
    "\n",
    "        with col2:\n",
    "            st.subheader(\"üí° Domains & Tools\")\n",
    "            tools = st.text_input(\"Tools / software (e.g., Power BI, Excel)\")\n",
    "            languages = st.text_input(\"Programming languages (e.g., Python, R, SQL)\")\n",
    "            frameworks = st.text_input(\"AI frameworks / libraries (e.g., Scikit-learn, Pandas)\")\n",
    "            data_types = st.text_input(\"Types of data handled (e.g., tabular, text, images)\")\n",
    "            preferred_domains = st.text_input(\"Preferred domains (e.g., finance, healthcare)\")\n",
    "        \n",
    "        st.subheader(\"üìù Describe Your Experience\")\n",
    "        experience_text = st.text_area(\"Provide a summary of your projects and professional experience.\", height=150)\n",
    "        challenges = st.text_area(\"What was the biggest challenge you faced in your projects, and how did you overcome it?\", height=150)\n",
    "        learning_goals = st.text_area(\"What skills or domains are you looking to improve or learn next?\", height=100)\n",
    "\n",
    "        submitted = st.form_submit_button(\"üîç Find My Job\")\n",
    "\n",
    "    if submitted:\n",
    "        if all([level_python, level_ai, level_visu, level_sql, level_token_embedding,\n",
    "                tools, languages, frameworks, data_types, preferred_domains,\n",
    "                experience_text, challenges, learning_goals\n",
    "                ]):\n",
    "        \n",
    "            if not all([tools, languages, frameworks, experience_text]):\n",
    "                st.warning(\"‚ö†Ô∏è Please fill in all the text fields for an accurate analysis!\")\n",
    "            else:\n",
    "                with st.spinner('Analyzing your profile...'):\n",
    "                    results = main.nlp(\n",
    "                        level_python, level_ai, level_visu, level_sql, level_token_embedding,\n",
    "                        tools, languages, frameworks, data_types, preferred_domains,\n",
    "                        experience_text, challenges, learning_goals\n",
    "                    )\n",
    "\n",
    "                if results:\n",
    "                    st.header(\"üìà Your Personalized Results Dashboard\")\n",
    "                    st.subheader(\"üèÜ Your Top 3 Job Recommendations\")\n",
    "                    cols = st.columns(3)\n",
    "                    for i, job in enumerate(results[\"top_jobs\"]):\n",
    "                        with cols[i]:\n",
    "                            st.metric(label=job['title'], value=f\"{job['score']}% Match\")\n",
    "                            with st.expander(\"Why this recommendation?\"):\n",
    "                                st.write(\"This role is a good fit because of your skills in:\")\n",
    "                                for skill in job['matching_skills']:\n",
    "                                    st.markdown(f\"- **{skill}**\")\n",
    "                    \n",
    "                    st.markdown(\"---\")\n",
    "\n",
    "                    col1, col2 = st.columns(2)\n",
    "                    with col1:\n",
    "                        st.subheader(\"üéØ Overall Profile Match\")\n",
    "                        st.plotly_chart(results[\"fig\"], use_container_width=True)\n",
    "                    with col2:\n",
    "                        st.subheader(\"üí° Competency Block Coverage\")\n",
    "                        st.write(\"This shows how well your profile covers different skill areas.\")\n",
    "                        block_names = list(results[\"block_scores\"].keys())\n",
    "                        block_values = list(results[\"block_scores\"].values())\n",
    "                        bar_fig = go.Figure([go.Bar(x=block_values, y=block_names, orientation='h', text=block_values, textposition='auto', marker_color='#4169E1')])\n",
    "                        bar_fig.update_layout(title=\"Coverage per Skill Category (%)\", xaxis_title=\"Coverage Score\", yaxis_title=\"Competency Block\")\n",
    "                        st.plotly_chart(bar_fig, use_container_width=True)\n",
    "                else:\n",
    "                    st.error(\"‚ùå Could not process your profile. Please check if the data files are available.\")\n",
    "        else : \n",
    "            submitted = False\n",
    "            st.warning(\"Please answer all the questions !!\")\n",
    "\n",
    "        \n",
    "start()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb847cf9",
   "metadata": {},
   "source": [
    "### Run the Streamlit Frontend Interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f4a42735",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    }
   ],
   "source": [
    "!streamlit run ../interface/front.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
